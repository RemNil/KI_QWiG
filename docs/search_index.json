[["index.html", "Dossier-Graph: A Knowledge Graph for German HTA reports Project summary", " Dossier-Graph: A Knowledge Graph for German HTA reports Merlin RemNil 30.07.2025 Project summary Dossier-Graph is an open-source and free knowledge graph system designed for research and education, built on public data from transparent health technology assessments (IQWiG 2023). Its mission: transform a vast archive of dossier evaluations into a live, searchable database using AI. Key Features Flexible, Traceable Linking: Connects dossiers, addenda, tables, sections, and more complex conceptual relationships. Ensures both human-readable and machine-consumable navigation paths. AI-Driven Search &amp; Analysis: Automates extraction, semantic search, and interactive analysis. Users ask questions in natural language and receive comprehensive, cross-document answers instantly. Why it matters Clinical development of new medicines differs from many business models because it relies critically on voluntary, even altruistic (Jansen 2009) research participation and hence on public trust. “Information gain per unit of accepted research burden” is a useful concept introduced to think about how regulatory oversight learns from past clinical R&amp;D efforts – a prerequisite of earning public trust. Information is gained not only from conducting clinical research but throughout the entire regulatory process, including post-marketing health technology assessment. Therefore, research responsibilities to maximize available information extend well beyond publishing study results. In this spirit, the Dossier-Graph knowledge graph is designed to transform post-marketing health technology assessment data into actionable insights, honoring the contributions of the many patients involved in the underlying research. By integrating state-of-the-art information technology throughout the drug development and regulatory lifecycle, this project advances the actionable use of critical research information. Found a mistake? Open peer-review is enabled in this project using hypothes.is. This allows sentence-by-sentence annotation from readers directly on this page. Please feel free to annotate. Both constructive and destructive criticism is highly welcome. References IQWiG. 2023. “Allgemeine Methoden; Version 7.0.” [online] Zugriff: 02.07.2025. https://www.iqwig.de/methoden/allgemeine-methoden_version-7-0.pdf. Jansen, Lynn A. 2009. “The Ethics of Altruism in Clinical Research.” Hastings Center Report 39 (4): 26–36. "],["introduction.html", "1 Introduction", " 1 Introduction What if we could see the research landscape not as scattered trials, but as a living map of inquiry—constantly evolving, purposefully connected, and ethically aligned? With an AI-powered knowledge graph trained on the vast archive of HTA reports, could we finally chart clinical studies in real time, uncovering how each trial fits into the bigger picture—and whether it truly earns the burden it places on participants? "],["clinical-trial-portfolio-analysis.html", "2 Clinical Trial Portfolio Analysis 2.1 Trial Portfolios and Research Trajectories 2.2 Limitations of hitorical data analysis", " 2 Clinical Trial Portfolio Analysis Ethical justification for the burdens placed on clinical research participants extends well beyond rigorous trial design. Modern scholarship stresses that trials should form part of an aligned research portfolio, where each study fills knowledge gaps and builds on prior evidence, contributing to a coherent, cumulative understanding of therapeutic questions. This portfolio approach is central for generating social value and ethically justifying participant burdens, as research efforts that are strategically linked offer a far greater return on participant contribution than isolated studies (London and Kimmelman 2019; Kimmelman, Carlisle, and Gönen 2017; Kimmelman and London 2015). Evaluating clinical research from a bird’s eye perspective, particularly after drug approval, produces two key benefits: Improving Future Trials: Analyses of existing research portfolios can highlight recurring challenges or risks; these insights directly inform the design, oversight, and safety of subsequent studies. For example, patterns identified at the portfolio level can help investigators anticipate methodological pitfalls or ethical concerns (Bittlinger, Peppercorn, and Kimmelman 2022). Revealing Research Gaps: Only by viewing groups of related trials together do unmet needs and new research questions rise to the surface—questions invisible to single-study analysis, but crucial for continuous improvement and targeted innovation (Kimmelman, Carlisle, and Gönen 2017; London and Kimmelman 2019). 1. Contextualizing Research Within the Scientific Continuum Interdependence of Questions: Each study should not exist in isolation. Instead, it should build upon previous findings and lay the groundwork for subsequent inquiry. This continuity ensures that research is not a series of disconnected efforts, but a tapestry of knowledge where each thread informs and supports the others. Cumulative Value of Evidence: The justification for involving participants must consider how the current research contributes to a growing body of evidence. Studies should be positioned to fill genuine knowledge gaps, refine existing understanding, or resolve uncertainties left by prior trials. 2. Strategic Question Selection and Research Prioritization Meaningful Sequencing: What we choose to ask next matters. The process of formulating research questions must be deliberate, ensuring they are timely, necessary, and logically connected to what has been already established. This sequencing maximizes the efficiency and impact of participant commitments. Avoiding Redundancy and Waste: By coordinating research agendas and sharing data openly, the field can prevent unnecessary replication and avoid subjecting participants to studies of limited incremental value. 3. Ethical Stewardship: Participant Burden in Perspective Value Amplification: When research questions are interconnected and aligned towards elucidating a coherent “bigger picture,” the societal value gained for each participant’s sacrifice increases. Their contributions are part of a collective effort, not a solitary, potentially obscure, experiment. Transparency and Trust: Clearly articulating how each study fits within the broader research mission fosters public trust, honors participant altruism, and reinforces the legitimacy of the research burden. 4. Cultivating a Healthy Research Culture Interdisciplinary Collaboration: Encouraging collaboration across disciplines and institutions ensures that research priorities are shaped by a diversity of perspectives and pressing clinical realities. Vision and Foresight: Healthy research ecosystems invest in horizon scanning—anticipating future questions and challenges—to guide present study designs and ethical considerations. 2.1 Trial Portfolios and Research Trajectories Let us examine some examples using EMA data from the EMA website. On the x-axis we plot time (years) and on the y-axis we plot the “volume” (counts) of EMA approvals per category (therapeutic areas). We do so using a stream graph (Byron and Wattenberg 2008), where the volume of individual streams (i.e. number of approvals per year and therapeutic area) is proportional to the values in each category. div.blue { background-color: #e6f0ff; border-radius: 5px; padding: 20px; margin-bottom: 10px; /* adds vertical space below the box */ } Key features of stream graphs Trends Over Time: Easily see rises and falls in activity for different categories (e.g., trial phases, therapeutic areas). Relative Volumes: The thickness of each stream at a given time denotes the volume (number of trials or approvals) for that category. Research Shifts: Highlight when attention moves from one research area to another, or when new categories emerge. It’s particularly powerful for showing the temporal development of research portfolios, such as how the number of clinical trials in different indications expand, contract, or shift across years. This plot visually presents “trends” in approval rate per therapeutic area over time. The width of each category’s stream fluctuates, letting us spot certain observations (e.g., surges in lung cancer trials after pembrolizumab’s first lung approval). Events like regulatory approvals or new indication launches appear as “bursts.” Declining or “sunset” indications shrink and eventually disappear, providing a living map of the research focus over time as described in the reference example This streamgraph visualizes trends in EMA regulatory approval categories from the early 2000s to 2024. Each colored stream represents one category, with the stream’s thickness indicating the number of medicines approved under that category each year. Interpretation by Category: Additional monitoring (largest stream): This category has grown substantially since 2012, peaking in 2022. It reflects increased post-market surveillance requirements for newer medicines, possibly due to evolving safety monitoring standards. Generic or hybrid: Steady presence over the years, with moderate peaks around 2009–2012 and a strong resurgence from 2021–2024. This suggests growing availability and approval of off-patent medicines or new combinations/formulations. Orphan medicine: Consistent upward trend, especially post-2018, reflecting EMA’s ongoing support for therapies targeting rare diseases. Biosimilar: Emerging around 2010 and slowly gaining momentum. The rise in recent years aligns with the expiration of patents on major biologics. Accelerated assessment, Advanced therapy, Conditional approval, PRIME: priority medicine: These smaller streams show limited but increasing use, indicating selective but strategic application of fast-track or specialized pathways for innovative or high-need treatments. In sum, the graph shows that regulatory pathways tailored for innovation, safety, and patient access have become more prominent. Particularly, Additional monitoring and Orphan medicine categories have seen major growth, while generics and biosimilars reflect market maturity and cost-containment policies. Applications div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} Portfolio Review: See if resources align with evolving scientific opportunities. Ethical Justification: Ensure new studies fill real knowledge gaps (not redundant with past research). Gap Analysis: Identify under-studied areas or indications ripe for new trials. 2.2 Limitations of hitorical data analysis Data Relevance and Timeliness: Historical data may not reflect recent changes in the environment, such as technological advancements, market disruptions, or shifts in consumer behavior. This can make insights obsolete or misleading. Quality and Completeness Issues: Historical datasets can suffer from inaccuracies, incompleteness, or inconsistent formats, reducing the reliability of any conclusions drawn. Survivorship Bias: The data might be affected by biases, such as focusing on entities that have survived or succeeded while ignoring those that have failed, leading to skewed interpretations (also known as survivorship bias). Limited Predictive Power: Historical patterns do not always guarantee future outcomes, especially in volatile environments or during unprecedented events. Unexpected disruptions (like the COVID-19 pandemic) often break with historical trends. Overfitting: There is a risk of creating models that fit historical data too closely but fail to generalize accurately to new situations or data—a problem especially in complex statistical modeling. Stifling Innovation: Exclusive reliance on past data can limit creativity, making organizations less likely to explore new approaches or adapt to novel circumstances. Cultural and Contextual Shifts: Societal, regulatory, or industry-specific changes can render historical data less useful for contemporary analysis, potentially leading to misaligned strategies References Bittlinger, Merlin, Jeffrey Peppercorn, and Jonathan Kimmelman. 2022. “Ethical Considerations for Phase I Trials in Oncology.” Journal of Clinical Oncology 40 (30): 3474–88. https://doi.org/10.1200/JCO.21.02125. Byron, Lee, and Martin Wattenberg. 2008. “Stacked Graphs–Geometry &amp; Aesthetics.” IEEE Transactions on Visualization and Computer Graphics 14 (6): 1245–52. Kimmelman, Jonathan, Benjamin Carlisle, and Mithat Gönen. 2017. “Drug Development at the Portfolio Level Is Important for Policy, Care Decisions and Human Protections.” Jama 318 (11): 1003–4. Kimmelman, Jonathan, and Alex John London. 2015. “The Structure of Clinical Translation: Efficiency, Information, and Ethics.” Hastings Center Report 45 (2): 27–39. London, Alex John, and Jonathan Kimmelman. 2019. “Clinical Trial Portfolios: A Critical Oversight in Human Research Ethics, Drug Regulation, and Policy.” Hastings Center Report 49 (4): 31–41. "],["knowledge-graph.html", "3 Knowledge Graph", " 3 Knowledge Graph div.blue { background-color: #e6f0ff; border-radius: 5px; padding: 20px; margin-bottom: 10px; /* adds vertical space below the box */ } Key features of Knowledge Graphs Makin it easier to reason across complex topics by connecting related information Bridging large datasets and multiple data sources (e.g., IQWIG benefit assessments, EMA EPARs, G-BA “Tragende Gründe” etc.). Facilitate easy information retrieval using Retrieval-Augmented Generation (RAG). 1. Data sources Data is in the public domain and can be webscraped for academic/educational noncommercial use (https://github.com/RemNil/Dossier-Graph) However, all source data is publish as PDF files. PDFs are difficult to work with because they store visual layout rather than structured content. Unlike Word or HTML, PDFs lack semantic tags like headings, paragraphs, or tables—they simply position text on a page. This makes it hard to extract meaningful data, especially from tables or multi-column layouts, since there’s no inherent reading order or logical structure. In contrast, formats like .docx, HTML, or JSON are designed for structured content and are far easier to parse programmatically. div.red { background-color: #FFE6F0; border-radius: 5px; padding: 20px; margin-bottom: 10px; /* adds vertical space below the box */ } Key limitations TL;DR: PDFs are designed for layout and visual fidelity, not structured data. That makes them painful to parse, especially when dealing with things like tables, multi-column layouts, footnotes, or inconsistent fonts. li&gt; 2. Data processing Parsing complex scientific documents like IQWiG reports requires more than generic PDF extraction — especially when dealing with long-form clinical evaluations, dense tabular data, and nuanced medical terminology. Unlike typical journal articles that follow a standardized structure (Abstract, Methods, Results, etc.), IQWiG reports are semi-structured, with institution-specific conventions, recurring section types, and domain-specific vocabulary. These characteristics necessitate a tailored, hybrid processing pipeline that combines layout-aware extraction with semantic parsing and structured data representation. IQWiG reports typically span dozens of pages (often 80+), and blend narrative analysis, evaluation of clinical trials, and formal assessments with dense tables, footnotes, and abbreviations. Off-the-shelf tools like GROBID — while excellent for standardized scientific literature — are too rigid for these documents, as they assume a publication layout seen in PubMed or conference papers. GROBID is not easily adaptable to custom headings, IQWiG-specific terminologies like “Beleg” or “Hinweis”, or non-standard section structures. Instead, a hybrid strategy proves far more effective. Tools like pdfplumber and PyMuPDF offer reliable low-level access to the visual structure of each page, allowing precise control over table extraction, bounding box analysis, and text block sequencing. Meanwhile, unstructured.io provides high-level chunking capabilities, converting narrative content into semantically tagged elements such as NarrativeText, Title, List, or Table. This dual approach allows the processing to be tailored per content type: text-heavy sections can be routed to semantic analysis, while table-heavy pages can be extracted, cleaned, and converted to structured formats like CSV or JSON. Given the volume and complexity of your corpus — 1,500 IQWiG PDFs — a modular pipeline architecture is essential. The strategy should begin by segmenting each document into page-level or section-level units, identifying which parts contain predominantly tabular data, and which contain explanatory or evaluative text. This content-type segmentation allows for targeted processing: table pages can be handled using pdfplumber or camelot with post-processing to normalize headers and cell values, while text sections are parsed with unstructured.io, producing rich, metadata-tagged chunks suitable for indexing, embedding, or further NLP. Your key processing goals define the pipeline’s features: Accurate Metadata Extraction: Each document should be parsed to extract bibliographic and structural metadata into a consistent JSON schema — including document title, project code, report type, section labels, page ranges, and table/figure references. Concept Annotation: Leveraging either spaCy (with medical or German-language models) or a domain-adapted LLM, key concepts such as medical terms, study identifiers, clinical endpoints, and IQWiG-evaluative language (&quot;Hinweis&quot;, &quot;Beleg&quot;, etc.) can be tagged. These annotations serve as candidates for a downstream knowledge graph. Abbreviation List Extraction: A glossary of abbreviations should be built by automatically detecting and parsing abbreviation-definition pairs from designated sections and table footnotes, aggregating terms across documents into a unified dictionary. Structural Hierarchy Preservation: Despite lacking standard journal layout, IQWiG reports do follow a templated structure, often with consistent headings and expected subtopics. The pipeline should use rule-based or ML-aided logic to detect section boundaries and maintain the document’s semantic hierarchy. This is crucial for maintaining links between a given table and the explanatory text surrounding it, as well as connecting conclusions with referenced studies or findings. Semantic Integration of Tables and Text: Tables in these documents are not standalone — they are tightly linked to claims, evidence categories, and footnote-based qualifiers. Your processing pipeline should be able to identify these cross-references, potentially through heuristics (e.g., detecting references like “siehe Tabelle 4”) or embedding-based similarity matching between tabular rows and narrative mentions. Overall, the goal is not just extraction, but understanding — to transform complex, long-form PDF reports into structured, semantically enriched data that supports both human and machine reasoning. This requires precision, modularity, and a pipeline that is aware of the domain’s unique characteristics. By using a hybrid strategy — layout-first tools for precision (like pdfplumber), and semantic-aware chunkers (like unstructured.io) — and by breaking down the documents into specialized processing flows for text, tables, metadata, and glossary content, your system can scale effectively across a large corpus while respecting the nuance of each report. 3. Graph development 4. Quality Assurance 5. Model deployment "],["concept-development-leveraging-large-empirical-data.html", "4 Concept development leveraging large empirical data 4.1 Rationale", " 4 Concept development leveraging large empirical data 4.1 Rationale Traditional concept development in research ethics involves the systematic analysis and refinement of ethical principles that guide the conduct of research involving human and animal subjects. Rooted in philosophical inquiry and bioethics, this approach typically focuses on foundational concepts such as autonomy, beneficence, non-maleficence, and justice. Scholars use conceptual analysis to clarify the meaning, scope, and application of these principles, often drawing from case studies, historical precedents, and normative ethical theories. The goal is to establish clear, coherent, and universally applicable ethical standards that inform research policies, institutional review board (IRB) guidelines, and the ethical training of researchers. In contrast to the traditional, normative approach in research ethics (which is largely conceptual and philosophical), integrating empirical knowledge—such as data from Health Technology Assessments (HTAs), clinical trial registries, or stakeholder surveys—is less common but increasingly recognized as valuable. While empirical methods are standard in clinical research for generating and refining questions, their use in research ethics is still emerging and sometimes viewed with caution. This is partly because research ethics has traditionally focused on what ought to be done, rather than what is happening. However, empirical ethics—a growing interdisciplinary field—argues that understanding actual practices, stakeholder experiences, and outcomes can enrich ethical reflection and make it more context-sensitive. For example, analyzing HTA data might reveal patterns of exclusion, inefficiencies, or unintended harms in research practices that raise new ethical concerns or highlight neglected issues. Thus, while it may be less common, empirical approaches can play a crucial role in identifying ethically relevant gaps and informing the evolution of ethical standards in clinical research. A strong example of such patterns becoming visible through a bird’s-eye view of clinical research—especially via analysis of large-scale data like from HTAs, trial registries, or systematic reviews—is the systematic underrepresentation of certain populations in clinical trials, such as: Example: Underrepresentation of Older Adults in Cancer Clinical Trials Insights from large-scale empirical data analysis: A: Bird’s-eye view through trial registries or meta-analyses shows that older adults (e.g., 65+) are consistently underrepresented in oncology trials, despite being the population most affected by cancer. B: This trend emerges only when you aggregate and analyze data across many studies. C: The reasons might include concerns about comorbidities, presumed frailty, or protocol exclusions—but these often go unchallenged until empirically documented. From a conceptual ethics standpoint, justice and fairness demand equitable access to research participation and the benefits of scientific advancement. But this normative claim doesn’t immediately reveal whether inequities are actually occurring in practice. Ethical implications: Unintended harms: Treatments are approved based on data from younger, healthier populations, leading to less safe or less effective care for older adults. Inefficiencies: Clinical research may not generalize well to the actual patient population, leading to wasted resources or repeat trials. Injustice: Systematic exclusion undermines the ethical principle of equitable access and can exacerbate health disparities. Without large-scale, empirical insight, these ethical issues remain diffuse or anecdotal. But with a data-informed view of the current state of research, they become actionable ethical concerns that can lead to refined inclusion criteria, new trial designs, or policy changes. "],["comprehensive-documention-dossier-graph.html", "5 Comprehensive Documention: Dossier-Graph", " 5 Comprehensive Documention: Dossier-Graph 5.0.1 Implementation steps Here is a stepy by step descriptopn of how to implement the Dossier-Graph on your system (assuming UBUNTU 24 LTS). "],["how-it-works-and-how-its-going.html", "6 How it works and how its going", " 6 How it works and how its going Veritas filia temporis. div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} This is my first conclusion This is my second conclusion Useful tools clinical trials viewer "],["references.html", "References", " References … you missed it. Please go back and click on some of the links for further reading… Bittlinger, Merlin, Jeffrey Peppercorn, and Jonathan Kimmelman. 2022. “Ethical Considerations for Phase I Trials in Oncology.” Journal of Clinical Oncology 40 (30): 3474–88. https://doi.org/10.1200/JCO.21.02125. Byron, Lee, and Martin Wattenberg. 2008. “Stacked Graphs–Geometry &amp; Aesthetics.” IEEE Transactions on Visualization and Computer Graphics 14 (6): 1245–52. IQWiG. 2023. “Allgemeine Methoden; Version 7.0.” [online] Zugriff: 02.07.2025. https://www.iqwig.de/methoden/allgemeine-methoden_version-7-0.pdf. Jansen, Lynn A. 2009. “The Ethics of Altruism in Clinical Research.” Hastings Center Report 39 (4): 26–36. Kimmelman, Jonathan, Benjamin Carlisle, and Mithat Gönen. 2017. “Drug Development at the Portfolio Level Is Important for Policy, Care Decisions and Human Protections.” Jama 318 (11): 1003–4. Kimmelman, Jonathan, and Alex John London. 2015. “The Structure of Clinical Translation: Efficiency, Information, and Ethics.” Hastings Center Report 45 (2): 27–39. London, Alex John, and Jonathan Kimmelman. 2019. “Clinical Trial Portfolios: A Critical Oversight in Human Research Ethics, Drug Regulation, and Policy.” Hastings Center Report 49 (4): 31–41. "]]
