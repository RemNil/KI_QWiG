# Comprehensive Documention: Dossier-Graph


## Implementation steps

Here is a step by step description of how to implement the Dossier-Graph on your system (assuming UBUNTU 24 LTS).

## Model Fine-Tuning to IQWiG Domain Knowledge

#### STEP 1:  Data Preparation
Convert HTA Reports to Training Examples
FOR each IQWiG report:
    extract_text_from_pdf(report.pdf)
    
    // Essential: Create question-answer pairs
    training_example = {
        question: "Summarize the benefit assessment for [drug name]"
        context: "IQWiG report A23-42: Methods section... Results section..."  
        answer: "The assessment concluded considerable additional benefit..."
    }
    
    // Essential: Use IQWiG's exact format
    format_as_conversation(training_example)
    // "[INST] Question + Context [/INST] Answer"

WHY ESSENTIAL: The AI learns by seeing thousands of examples of "good HTA writing"

#### STEP 2: Model Setup
Load Base AI Model
base_model = load_mistral_7b()  // 7 billion parameters
compress_model_to_4bit()        // Reduces memory from 28GB to 7GB


#### STEP 3: Apply LoRA Modification
// Essential: Only modify specific parts, not entire model
target_layers = ["attention_weights", "output_projections"] 
FOR each target_layer:
    add_small_adjustment_matrix(layer, size=64)  // 64 parameters vs 4 million

WHY ESSENTIAL: We're not retraining the entire AI (impossible), just teaching it 
HTA-specific patterns by adding tiny "adjustment dials"


#### STEP 4: Training Loop
FOR 3 complete_passes_through_data:
    FOR each training_example:
        
        // Feed the AI the question + context
        prediction = model.generate_answer(question + context)
        
        // Compare to correct HTA answer
        error = calculate_difference(prediction, correct_answer)
        
        // Essential: Adjust only the LoRA parameters
        adjust_small_matrices(error_signal)
        // Original model stays frozen!
        
#### STEP 5: Validation  
EVERY 100 examples:
    test_on_holdout_reports()
    IF error_increasing:
        stop_training()  // Prevent overfitting

WHY ESSENTIAL: Like teaching someone to write in IQWiG style - show examples, 
give feedback, adjust gradually

#### STEP 6: Using the Trained Model
user_question = "Assess the evidence quality for pembrolizumab in melanoma"
user_context = "Single RCT, n=834 patients, 22-month follow-up..."

// Essential: Format exactly like training
formatted_input = "[INST] " + user_question + "\n\nContext: " + user_context + " [/INST]"

// Model processing (simplified)
tokens = convert_text_to_numbers(formatted_input)
FOR each position in sequence:
    // Base model generates medical knowledge
    base_probabilities = mistral_base_model(tokens)
    
    // LoRA adjustments add HTA-specific patterns  
    hta_adjustments = lora_matrices(tokens)
    final_probabilities = base_probabilities + hta_adjustments
    
    next_word = sample_from_probabilities(final_probabilities)
    tokens.append(next_word)

response = convert_numbers_to_text(tokens)

WHY ESSENTIAL: The base model provides general medical knowledge, 
LoRA adds IQWiG-specific writing patterns